{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb10931",
   "metadata": {},
   "source": [
    "# 3. Fun√ß√µes Utilizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5665d3",
   "metadata": {},
   "source": [
    "## 3.1. An√°lises gerais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85af464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_duplicatas(df, col_id=\"msno\", col_safra=\"safra\", visualizar=False):\n",
    "    \"\"\"\n",
    "    Identifica e conta linhas duplicadas para a mesma chave (ID + Safra).\n",
    "    \"\"\"\n",
    "    # Define a janela abrangendo todas as linhas do grupo\n",
    "    window_spec = Window.partitionBy(col_id, col_safra)\\\n",
    "                        .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # Adiciona contagem e filtra duplicados\n",
    "    df_duplicados = df.withColumn(\"contagem_janela\", F.count(\"*\").over(window_spec)).filter(\"contagem_janela > 1\")\n",
    "    \n",
    "    if visualizar:\n",
    "        df_duplicados.show(10)\n",
    "    \n",
    "    # Retorna a quantidade total de linhas duplicadas encontradas\n",
    "    return df_duplicados.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_mudanca_estado(df, col_alvo, col_id=\"msno\", col_safra=\"safra\", visualizar=False):\n",
    "    \"\"\"\n",
    "    Verifica mudan√ßas de estado em uma vari√°vel ao longo do tempo (safras).\n",
    "    \"\"\"\n",
    "    # Definir a especifica√ß√£o da janela\n",
    "    window_spec = Window.partitionBy(col_id).orderBy(col_safra)\n",
    "\n",
    "    # Lag para criar as colunas de estado anterior da variavel e detec√ß√£o de mudan√ßa\n",
    "    df_historico = (\n",
    "        df.withColumn(f\"{col_alvo}_anterior\", F.lag(col_alvo).over(window_spec))\n",
    "          .withColumn(\"mudou\", \n",
    "              F.when(F.col(f\"{col_alvo}_anterior\").isNull(), False)\n",
    "               .otherwise(F.col(col_alvo) != F.col(f\"{col_alvo}_anterior\"))))\n",
    "\n",
    "    # Caso o usuario deseje visualizar as mudancas\n",
    "    if visualizar:\n",
    "        # Mostrar quantas mudancas ocorreram por safra\n",
    "        print(\"Contagem de mudan√ßas por safra:\")\n",
    "        df_historico.filter(F.col(\"mudou\") == True).groupBy(col_safra, \"mudou\").count().orderBy(col_safra, \"mudou\").show(30, truncate=False)\n",
    "        # Mostrar quantas vezes o usuario mudou\n",
    "        print(\"Mudan√ßas por usu√°rio:\")\n",
    "        df_historico.filter(F.col(\"mudou\") == True).groupBy(col_id).agg(F.sum(F.col(\"mudou\").cast(\"int\")).alias(\"total_mudancas\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_distribuicao(df, colunas_alvo, col_id=\"msno\", n_show=10, agrupar_por_safra=False):\n",
    "    \"\"\"\n",
    "    Gera agrega√ß√£o de contagem e percentual sobre uma vari√°vel alvo.\n",
    "    \"\"\"\n",
    "    # 1. Calcula o total da base para o percentual global\n",
    "    total_base = df.count()\n",
    "\n",
    "    if agrupar_por_safra:\n",
    "        # 2. Agrupamento inicial por safra e alvo\n",
    "        df_result = df.groupBy(\"safra\", *colunas_alvo).agg(\n",
    "            F.count(col_id).alias(\"total\")\n",
    "        )\n",
    "        \n",
    "        # 3. Define a janela para calcular o total por safra (denominador do pct_safra)\n",
    "        window_safra = Window.partitionBy(\"safra\")\n",
    "        \n",
    "        # 4. Adiciona os c√°lculos de percentual\n",
    "        df_result = df_result.withColumn(\n",
    "            \"pct_safra\", \n",
    "            F.round((F.col(\"total\") / F.sum(\"total\").over(window_safra)) * 100, 2)\n",
    "        ).withColumn(\n",
    "            \"pct_total\", \n",
    "            F.round((F.col(\"total\") / F.lit(total_base)) * 100, 2)\n",
    "        ).orderBy(\"safra\", F.desc(\"total\"))\n",
    "        \n",
    "    else:\n",
    "        # Resultado geral sem parti√ß√£o de safra\n",
    "        df_result = df.groupBy(colunas_alvo).agg(\n",
    "            F.count(col_id).alias(\"total\"),\n",
    "            F.round((F.count(col_id) / F.lit(total_base)) * 100, 2).alias(\"pct_total\")\n",
    "        ).orderBy(F.desc(\"total\"))\n",
    "    \n",
    "    df_result.show(n_show, truncate=False)\n",
    "    return df_result # Retornar o DF √© uma boa pr√°tica para an√°lises posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad65673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(df, colunas, plot=True, top_n=15, figsize=(20, 8)):\n",
    "    \"\"\"\n",
    "    Calcula e visualiza matriz de correla√ß√£o com layout otimizado.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    df : Spark DataFrame\n",
    "    colunas : Lista com nomes das colunas num√©ricas\n",
    "    plot : bool - Se True, gera visualiza√ß√£o\n",
    "    top_n : int - N√∫mero m√°ximo de features a exibir (ordenadas por vari√¢ncia ou IV)\n",
    "    figsize : tuple - Tamanho da figura\n",
    "    \"\"\"\n",
    "    n = len(colunas)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CASO 1: Par de vari√°veis (Scatter Plot)\n",
    "    # ========================================================================\n",
    "    if n == 2:\n",
    "        col1, col2 = colunas\n",
    "        \n",
    "        pearson_val = df.corr(col1, col2, method=\"pearson\")\n",
    "        spearman_val = df.corr(col1, col2, method=\"spearman\")\n",
    "        \n",
    "        print(f\"üìä Correla√ß√£o de Pearson:  {pearson_val:+.4f}\")\n",
    "        print(f\"üìä Correla√ß√£o de Spearman: {spearman_val:+.4f}\")\n",
    "\n",
    "        if plot:\n",
    "            amostra_pd = df.select(col1, col2).sample(\n",
    "                False, \n",
    "                fraction=min(1.0, 50000/df.count())\n",
    "            ).toPandas()\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 7))\n",
    "            \n",
    "            # Scatter com densidade\n",
    "            scatter = ax.scatter(\n",
    "                amostra_pd[col1], \n",
    "                amostra_pd[col2], \n",
    "                alpha=0.4, \n",
    "                s=20,\n",
    "                c=amostra_pd[col1],\n",
    "                cmap='viridis',\n",
    "                edgecolors='none'\n",
    "            )\n",
    "            \n",
    "            # Linha de tend√™ncia\n",
    "            z = np.polyfit(amostra_pd[col1], amostra_pd[col2], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(\n",
    "                amostra_pd[col1].sort_values(), \n",
    "                p(amostra_pd[col1].sort_values()), \n",
    "                \"r--\", \n",
    "                alpha=0.8, \n",
    "                linewidth=2,\n",
    "                label=f'Tend√™ncia: y = {z[0]:.3f}x + {z[1]:.3f}'\n",
    "            )\n",
    "            \n",
    "            ax.set_xlabel(col1, fontsize=12, fontweight='bold')\n",
    "            ax.set_ylabel(col2, fontsize=12, fontweight='bold')\n",
    "            ax.set_title(\n",
    "                f\"Dispers√£o: {col1} vs {col2}\\n\"\n",
    "                f\"Pearson: {pearson_val:+.3f}  |  Spearman: {spearman_val:+.3f}\",\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                pad=20\n",
    "            )\n",
    "            ax.grid(True, alpha=0.3, linestyle='--')\n",
    "            ax.legend(loc='best', fontsize=10)\n",
    "            \n",
    "            plt.colorbar(scatter, ax=ax, label=col1)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # CASO 2: Matriz de correla√ß√£o (> 2 vari√°veis)\n",
    "    # ========================================================================\n",
    "    else:\n",
    "        # Limitar n√∫mero de features se necess√°rio\n",
    "        if n > top_n:\n",
    "            print(f\"‚ö†Ô∏è  Muitas features ({n}). Exibindo apenas top {top_n} por vari√¢ncia.\")\n",
    "            \n",
    "            # Calcular vari√¢ncia de cada coluna\n",
    "            variancias = []\n",
    "            for col in colunas:\n",
    "                var = df.select(F.variance(col)).collect()[0][0]\n",
    "                variancias.append((col, var if var else 0))\n",
    "            \n",
    "            # Ordenar por vari√¢ncia e pegar top_n\n",
    "            colunas_top = [col for col, _ in sorted(variancias, key=lambda x: x[1], reverse=True)[:top_n]]\n",
    "        else:\n",
    "            colunas_top = colunas\n",
    "        \n",
    "        # Preparar vetor\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=colunas_top, \n",
    "            outputCol=\"features\", \n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        df_vector = assembler.transform(df).select(\"features\")\n",
    "        \n",
    "        # Calcular matrizes\n",
    "        def get_corr_matrix(method):\n",
    "            matrix = Correlation.corr(df_vector, \"features\", method=method).collect()[0][0]\n",
    "            return pd.DataFrame(matrix.toArray(), index=colunas_top, columns=colunas_top)\n",
    "        \n",
    "        print(\"üîÑ Calculando matriz de Pearson...\")\n",
    "        matrix_pearson = get_corr_matrix(\"pearson\")\n",
    "        \n",
    "        print(\"üîÑ Calculando matriz de Spearman...\")\n",
    "        matrix_spearman = get_corr_matrix(\"spearman\")\n",
    "        \n",
    "        if plot:\n",
    "            # ============================================================\n",
    "            # VISUALIZA√á√ÉO OTIMIZADA\n",
    "            # ============================================================\n",
    "            \n",
    "            # Truncar nomes longos para melhor legibilidade\n",
    "            labels_curtos = [\n",
    "                col[:30] + '...' if len(col) > 30 else col \n",
    "                for col in colunas_top\n",
    "            ]\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            # -------------------- PEARSON --------------------\n",
    "            mask_pearson = np.triu(np.ones_like(matrix_pearson, dtype=bool), k=1)\n",
    "            \n",
    "            sns.heatmap(\n",
    "                matrix_pearson,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap='RdBu_r',  # Vermelho = positivo, Azul = negativo\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\n",
    "                    \"shrink\": 0.8,\n",
    "                    \"label\": \"Correla√ß√£o\"\n",
    "                },\n",
    "                ax=axes[0],\n",
    "                xticklabels=labels_curtos,\n",
    "                yticklabels=labels_curtos,\n",
    "                annot_kws={\"size\": 8}\n",
    "            )\n",
    "            \n",
    "            axes[0].set_title(\n",
    "                \"Matriz de Pearson\\n(Correla√ß√£o Linear)\",\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                pad=15\n",
    "            )\n",
    "            axes[0].set_xticklabels(\n",
    "                axes[0].get_xticklabels(),\n",
    "                rotation=45,\n",
    "                ha='right',\n",
    "                fontsize=9\n",
    "            )\n",
    "            axes[0].set_yticklabels(\n",
    "                axes[0].get_yticklabels(),\n",
    "                rotation=0,\n",
    "                fontsize=9\n",
    "            )\n",
    "            \n",
    "            # -------------------- SPEARMAN --------------------\n",
    "            mask_spearman = np.triu(np.ones_like(matrix_spearman, dtype=bool), k=1)\n",
    "            \n",
    "            sns.heatmap(\n",
    "                matrix_spearman,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap='PiYG',  # Rosa/Verde para Spearman\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\n",
    "                    \"shrink\": 0.8,\n",
    "                    \"label\": \"Correla√ß√£o\"\n",
    "                },\n",
    "                ax=axes[1],\n",
    "                xticklabels=labels_curtos,\n",
    "                yticklabels=labels_curtos,\n",
    "                annot_kws={\"size\": 8}\n",
    "            )\n",
    "            \n",
    "            axes[1].set_title(\n",
    "                \"Matriz de Spearman\\n(Correla√ß√£o Monot√¥nica)\",\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                pad=15\n",
    "            )\n",
    "            axes[1].set_xticklabels(\n",
    "                axes[1].get_xticklabels(),\n",
    "                rotation=45,\n",
    "                ha='right',\n",
    "                fontsize=9\n",
    "            )\n",
    "            axes[1].set_yticklabels(\n",
    "                axes[1].get_yticklabels(),\n",
    "                rotation=0,\n",
    "                fontsize=9\n",
    "            )\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            # ============================================================\n",
    "            # VISUALIZA√á√ÉO ALTERNATIVA: Apenas Tri√¢ngulo Superior\n",
    "            # ============================================================\n",
    "            \n",
    "            print(\"\\nüìä Gerando visualiza√ß√£o alternativa (tri√¢ngulo superior)...\\n\")\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "            \n",
    "            # M√°scara para tri√¢ngulo superior\n",
    "            mask = np.triu(np.ones_like(matrix_pearson, dtype=bool))\n",
    "            \n",
    "            # Pearson (tri√¢ngulo inferior)\n",
    "            sns.heatmap(\n",
    "                matrix_pearson,\n",
    "                mask=mask,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap='RdBu_r',\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                ax=axes[0],\n",
    "                xticklabels=labels_curtos,\n",
    "                yticklabels=labels_curtos,\n",
    "                annot_kws={\"size\": 8}\n",
    "            )\n",
    "            \n",
    "            axes[0].set_title(\n",
    "                \"Pearson (Tri√¢ngulo Inferior)\",\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                pad=15\n",
    "            )\n",
    "            axes[0].set_xticklabels(\n",
    "                axes[0].get_xticklabels(),\n",
    "                rotation=45,\n",
    "                ha='right',\n",
    "                fontsize=9\n",
    "            )\n",
    "            axes[0].set_yticklabels(\n",
    "                axes[0].get_yticklabels(),\n",
    "                rotation=0,\n",
    "                fontsize=9\n",
    "            )\n",
    "            \n",
    "            # Spearman (tri√¢ngulo inferior)\n",
    "            sns.heatmap(\n",
    "                matrix_spearman,\n",
    "                mask=mask,\n",
    "                annot=True,\n",
    "                fmt=\".2f\",\n",
    "                cmap='PiYG',\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                square=True,\n",
    "                linewidths=0.5,\n",
    "                cbar_kws={\"shrink\": 0.8},\n",
    "                ax=axes[1],\n",
    "                xticklabels=labels_curtos,\n",
    "                yticklabels=labels_curtos,\n",
    "                annot_kws={\"size\": 8}\n",
    "            )\n",
    "            \n",
    "            axes[1].set_title(\n",
    "                \"Spearman (Tri√¢ngulo Inferior)\",\n",
    "                fontsize=14,\n",
    "                fontweight='bold',\n",
    "                pad=15\n",
    "            )\n",
    "            axes[1].set_xticklabels(\n",
    "                axes[1].get_xticklabels(),\n",
    "                rotation=45,\n",
    "                ha='right',\n",
    "                fontsize=9\n",
    "            )\n",
    "            axes[1].set_yticklabels(\n",
    "                axes[1].get_yticklabels(),\n",
    "                rotation=0,\n",
    "                fontsize=9\n",
    "            )\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        return matrix_pearson, matrix_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_clusters(matrix_pearson, threshold=0.85, figsize=(16, 12), max_pairs=30):\n",
    "    \"\"\"\n",
    "    Visualiza apenas pares com alta correla√ß√£o (|r| >= threshold).\n",
    "    Vers√£o otimizada para evitar sobreposi√ß√£o de labels.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    -----------\n",
    "    matrix_pearson : pd.DataFrame - Matriz de correla√ß√£o\n",
    "    threshold : float - Threshold m√≠nimo de correla√ß√£o\n",
    "    figsize : tuple - Tamanho da figura\n",
    "    max_pairs : int - N√∫mero m√°ximo de pares a exibir\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extrair pares com alta correla√ß√£o\n",
    "    pares_altos = []\n",
    "    n = len(matrix_pearson)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            corr_val = matrix_pearson.iloc[i, j]\n",
    "            if abs(corr_val) >= threshold:\n",
    "                pares_altos.append({\n",
    "                    'feature_1': matrix_pearson.index[i],\n",
    "                    'feature_2': matrix_pearson.columns[j],\n",
    "                    'correlacao': corr_val\n",
    "                })\n",
    "    \n",
    "    if not pares_altos:\n",
    "        print(f\"‚úÖ Nenhum par com |correla√ß√£o| >= {threshold}\")\n",
    "        return\n",
    "    \n",
    "    # Criar DataFrame e ordenar\n",
    "    df_pares = pd.DataFrame(pares_altos).sort_values('correlacao', key=abs, ascending=False)\n",
    "    \n",
    "    # Limitar n√∫mero de pares se necess√°rio\n",
    "    if len(df_pares) > max_pairs:\n",
    "        print(f\"‚ö†Ô∏è  Limitando visualiza√ß√£o aos top {max_pairs} pares (de {len(df_pares)} encontrados)\")\n",
    "        df_pares = df_pares.head(max_pairs)\n",
    "    \n",
    "    print(f\"\\nüîó Encontrados {len(pares_altos)} pares com |correla√ß√£o| >= {threshold}\\n\")\n",
    "    print(df_pares.to_string(index=False))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # VISUALIZA√á√ÉO MELHORADA\n",
    "    # ========================================================================\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Criar labels mais limpos\n",
    "    def truncar_nome(nome, max_len=35):\n",
    "        \"\"\"Trunca nome mantendo in√≠cio e fim\"\"\"\n",
    "        if len(nome) <= max_len:\n",
    "            return nome\n",
    "        else:\n",
    "            # Manter in√≠cio e fim\n",
    "            metade = (max_len - 3) // 2\n",
    "            return f\"{nome[:metade]}...{nome[-metade:]}\"\n",
    "    \n",
    "    # Labels formatados (um por linha, sem \"vs\")\n",
    "    labels = []\n",
    "    for _, row in df_pares.iterrows():\n",
    "        feat1 = truncar_nome(row['feature_1'], max_len=40)\n",
    "        feat2 = truncar_nome(row['feature_2'], max_len=40)\n",
    "        labels.append(f\"{feat1}\\n  ‚ü∑  {feat2}\")\n",
    "    \n",
    "    # Cores por intensidade\n",
    "    def get_color(corr):\n",
    "        abs_corr = abs(corr)\n",
    "        if abs_corr >= 0.99:\n",
    "            return '#8B0000'  # Vermelho escuro (perfeita)\n",
    "        elif abs_corr >= 0.95:\n",
    "            return '#DC143C'  # Vermelho (cr√≠tica)\n",
    "        elif abs_corr >= 0.90:\n",
    "            return '#FF8C00'  # Laranja (muito alta)\n",
    "        else:\n",
    "            return '#32CD32'  # Verde (alta)\n",
    "    \n",
    "    colors = [get_color(c) for c in df_pares['correlacao']]\n",
    "    \n",
    "    # Criar barras horizontais\n",
    "    y_pos = np.arange(len(df_pares))\n",
    "    bars = ax.barh(y_pos, df_pares['correlacao'], color=colors, alpha=0.85, height=0.7)\n",
    "    \n",
    "    # Configurar eixos\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(labels, fontsize=9, family='monospace')\n",
    "    ax.set_xlabel('Correla√ß√£o de Pearson', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlim([-1.05, 1.15])  # Espa√ßo extra para valores\n",
    "    \n",
    "    # T√≠tulo\n",
    "    ax.set_title(\n",
    "        f'Pares com Alta Correla√ß√£o (|r| ‚â• {threshold})\\n'\n",
    "        f'Total: {len(pares_altos)} pares | Exibindo: {len(df_pares)} pares',\n",
    "        fontsize=15,\n",
    "        fontweight='bold',\n",
    "        pad=20\n",
    "    )\n",
    "    \n",
    "    # Linhas de refer√™ncia\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=1.2, alpha=0.7)\n",
    "    ax.axvline(x=threshold, color='red', linestyle='--', linewidth=1.5, alpha=0.4, \n",
    "               label=f'Threshold = ¬±{threshold}')\n",
    "    ax.axvline(x=-threshold, color='red', linestyle='--', linewidth=1.5, alpha=0.4)\n",
    "    ax.axvline(x=0.95, color='orange', linestyle=':', linewidth=1.2, alpha=0.4, \n",
    "               label='Cr√≠tico = ¬±0.95')\n",
    "    ax.axvline(x=-0.95, color='orange', linestyle=':', linewidth=1.2, alpha=0.4)\n",
    "    \n",
    "    # Grid\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, (bar, val) in enumerate(zip(bars, df_pares['correlacao'])):\n",
    "        # Posi√ß√£o do texto\n",
    "        x_pos = val + 0.03 if val > 0 else val - 0.03\n",
    "        ha = 'left' if val > 0 else 'right'\n",
    "        \n",
    "        # Cor do texto (branco se barra escura)\n",
    "        text_color = 'white' if abs(val) >= 0.95 else 'black'\n",
    "        \n",
    "        # Texto dentro da barra se correla√ß√£o alta\n",
    "        if abs(val) >= 0.90:\n",
    "            x_pos = val - 0.03 if val > 0 else val + 0.03\n",
    "            ha = 'right' if val > 0 else 'left'\n",
    "            text_color = 'white'\n",
    "        \n",
    "        ax.text(\n",
    "            x_pos,\n",
    "            i,\n",
    "            f'{val:.3f}',\n",
    "            va='center',\n",
    "            ha=ha,\n",
    "            fontsize=10,\n",
    "            fontweight='bold',\n",
    "            color=text_color\n",
    "        )\n",
    "    \n",
    "    # Legenda\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#8B0000', label='Perfeita (‚â• 0.99)'),\n",
    "        Patch(facecolor='#DC143C', label='Cr√≠tica (0.95 - 0.99)'),\n",
    "        Patch(facecolor='#FF8C00', label='Muito Alta (0.90 - 0.95)'),\n",
    "        Patch(facecolor='#32CD32', label='Alta (0.85 - 0.90)'),\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=legend_elements,\n",
    "        loc='lower right',\n",
    "        fontsize=10,\n",
    "        framealpha=0.9,\n",
    "        title='Intensidade'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # TABELA RESUMIDA POR CATEGORIA\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"üìä RESUMO POR CATEGORIA DE CORRELA√á√ÉO\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    categorias = [\n",
    "        ('üî¥ PERFEITA (‚â• 0.99)', 0.99, 1.01),\n",
    "        ('üü† CR√çTICA (0.95 - 0.99)', 0.95, 0.99),\n",
    "        ('üü° MUITO ALTA (0.90 - 0.95)', 0.90, 0.95),\n",
    "        ('üü¢ ALTA (0.85 - 0.90)', 0.85, 0.90),\n",
    "    ]\n",
    "    \n",
    "    for titulo, min_val, max_val in categorias:\n",
    "        subset = df_pares[\n",
    "            (df_pares['correlacao'].abs() >= min_val) & \n",
    "            (df_pares['correlacao'].abs() < max_val)\n",
    "        ]\n",
    "        \n",
    "        if len(subset) > 0:\n",
    "            print(f\"\\n{titulo} ({len(subset)} pares)\")\n",
    "            print(\"-\"*100)\n",
    "            \n",
    "            for idx, row in subset.iterrows():\n",
    "                feat1 = row['feature_1'][:45]\n",
    "                feat2 = row['feature_2'][:45]\n",
    "                corr = row['correlacao']\n",
    "                \n",
    "                print(f\"  ‚Ä¢ {feat1:47s} ‚ü∑  {feat2:47s}  ‚îÇ  r = {corr:+.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_cramer_v(df, col1, col2):\n",
    "    # Verifica√ß√£o de seguran√ßa: n√£o rodar em colunas com muitos valores √∫nicos (num√©ricas)\n",
    "    # Se a coluna tiver mais de 100 valores distintos, provavelmente n√£o √© uma categoria √∫til\n",
    "    distinct_count = df.select(col1).distinct().count()\n",
    "    if distinct_count > 100:\n",
    "        print(f\"Pulei {col1}: muitos valores √∫nicos ({distinct_count}). Verifique se √© quantitativa.\")\n",
    "        return np.nan\n",
    "\n",
    "    # Indexa√ß√£o\n",
    "    idx1, idx2 = col1 + \"_idx\", col2 + \"_idx\"\n",
    "    df_indexed = StringIndexer(inputCol=col1, outputCol=idx1, handleInvalid=\"skip\").fit(df).transform(df)\n",
    "    df_indexed = StringIndexer(inputCol=col2, outputCol=idx2, handleInvalid=\"skip\").fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "    # Assembler para o Teste\n",
    "    assembler = VectorAssembler(inputCols=[idx1], outputCol=\"features\")\n",
    "    test_data = assembler.transform(df_indexed).select(\"features\", idx2)\n",
    "\n",
    "    # Chi-Square Test\n",
    "    res = ChiSquareTest.test(test_data, \"features\", idx2).head()\n",
    "    \n",
    "    # --- CORRE√á√ÉO DO ACESSO AOS ATRIBUTOS ---\n",
    "    chi2 = float(res.statistics[0]) # Estat√≠stica Chi2 correta\n",
    "    n = test_data.count()\n",
    "    \n",
    "    # Resgate do n√∫mero de categorias pelos metadados ou contagem\n",
    "    r = df_indexed.select(idx1).distinct().count()\n",
    "    k = df_indexed.select(idx2).distinct().count()\n",
    "\n",
    "    if n == 0 or min(r-1, k-1) <= 0: return 0.0\n",
    "    \n",
    "    v = np.sqrt(chi2 / (n * min(r-1, k-1)))\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4394ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_cramer_v_spark_optimized(df, colunas_categoricas, figsize=(20, 16)):\n",
    "    \"\"\"\n",
    "    Constr√≥i uma matriz de correla√ß√£o Cramer's V para vari√°veis categ√≥ricas.\n",
    "    Otimizada para visualiza√ß√£o de at√© 20+ vari√°veis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pyspark.sql.DataFrame\n",
    "        DataFrame com as vari√°veis categ√≥ricas\n",
    "    colunas_categoricas : list\n",
    "        Lista de nomes das colunas categ√≥ricas\n",
    "    figsize : tuple\n",
    "        Tamanho da figura (width, height)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Matriz de correla√ß√£o Cramer's V\n",
    "    \"\"\"\n",
    "    \n",
    "    n_cols = len(colunas_categoricas)\n",
    "    matrix = np.ones((n_cols, n_cols))\n",
    "    \n",
    "    print(f\"üîÑ Iniciando indexa√ß√£o de {n_cols} colunas categ√≥ricas...\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # 1. INDEXA√á√ÉO EM BATCH (Performance)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    indexers = [\n",
    "        StringIndexer(\n",
    "            inputCol=c, \n",
    "            outputCol=c+\"_idx\", \n",
    "            handleInvalid=\"keep\"  # Mant√©m categorias novas como \"unknown\"\n",
    "        ) \n",
    "        for c in colunas_categoricas\n",
    "    ]\n",
    "    \n",
    "    df_indexed = df\n",
    "    models = []\n",
    "    for idx, indexer in enumerate(indexers):\n",
    "        model = indexer.fit(df_indexed)\n",
    "        df_indexed = model.transform(df_indexed)\n",
    "        models.append(model)\n",
    "    \n",
    "    print(f\"‚úÖ Indexa√ß√£o conclu√≠da!\\n\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # 2. C√ÅLCULO DO CRAMER'S V (Tri√¢ngulo Superior)\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    total_pairs = (n_cols * (n_cols - 1)) // 2\n",
    "    current_pair = 0\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        for j in range(i + 1, n_cols):\n",
    "            current_pair += 1\n",
    "            col1 = colunas_categoricas[i]\n",
    "            col2 = colunas_categoricas[j]\n",
    "            col1_idx = col1 + \"_idx\"\n",
    "            col2_idx = col2 + \"_idx\"\n",
    "            \n",
    "            print(f\"Calculando: {col1} ‚Üî {col2}\")\n",
    "            \n",
    "            # Preparar dados para ChiSquareTest\n",
    "            assembler = VectorAssembler(inputCols=[col1_idx], outputCol=\"features\")\n",
    "            test_data = assembler.transform(df_indexed).select(\"features\", col2_idx)\n",
    "            \n",
    "            # Executar teste Qui-Quadrado\n",
    "            try:\n",
    "                res = ChiSquareTest.test(test_data, \"features\", col2_idx).head()\n",
    "                chi2 = float(res.statistics[0])\n",
    "                n = test_data.count()\n",
    "                \n",
    "                # N√∫mero de categorias (r e k)\n",
    "                r = len(models[i].labels)\n",
    "                k = len(models[j].labels)\n",
    "                \n",
    "                # C√°lculo do V de Cramer\n",
    "                if n > 0 and min(r-1, k-1) > 0:\n",
    "                    v = np.sqrt(chi2 / (n * min(r-1, k-1)))\n",
    "                else:\n",
    "                    v = 0.0\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è Erro ao calcular {col1} vs {col2}: {e}\")\n",
    "                v = 0.0\n",
    "                \n",
    "            # Preencher matriz (sim√©trica)\n",
    "            matrix[i, j] = v\n",
    "            matrix[j, i] = v\n",
    "    \n",
    "    print(f\"\\n‚úÖ C√°lculo conclu√≠do!\\n\")\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # 3. CRIAR DATAFRAME E VISUALIZA√á√ÉO OTIMIZADA\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    cramer_df = pd.DataFrame(\n",
    "        matrix, \n",
    "        index=colunas_categoricas, \n",
    "        columns=colunas_categoricas\n",
    "    )\n",
    "    \n",
    "    # Plotar Heatmap Otimizado\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # M√°scara para o tri√¢ngulo superior (opcional, deixa mais limpo)\n",
    "    mask = np.triu(np.ones_like(cramer_df, dtype=bool), k=1)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cramer_df, \n",
    "        annot=True,           # Mostrar valores\n",
    "        fmt='.2f',            # 2 casas decimais\n",
    "        cmap='RdYlGn_r',      # Colormap: Vermelho (alta correla√ß√£o) -> Verde (baixa)\n",
    "        vmin=0, \n",
    "        vmax=1,\n",
    "        center=0.5,           # Centro da escala\n",
    "        square=True,          # C√©lulas quadradas\n",
    "        linewidths=0.5,       # Linhas entre c√©lulas\n",
    "        cbar_kws={\n",
    "            \"shrink\": 0.8,\n",
    "            \"label\": \"Cramer's V\"\n",
    "        },\n",
    "        mask=mask,            # Oculta tri√¢ngulo superior (opcional)\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    # Ajustes de visualiza√ß√£o\n",
    "    plt.title(\n",
    "        \"Matriz de Associa√ß√£o: Cramer's V (Vari√°veis Categ√≥ricas)\", \n",
    "        fontsize=16, \n",
    "        fontweight='bold',\n",
    "        pad=20\n",
    "    )\n",
    "    plt.xlabel(\"\", fontsize=12)\n",
    "    plt.ylabel(\"\", fontsize=12)\n",
    "    \n",
    "    # Rotacionar labels para melhor legibilidade\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cramer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74939142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz_cramer_v_spark(df, colunas_categoricas):\n",
    "    \"\"\"\n",
    "    Constr√≥i uma matriz de correla√ß√£o Cramer's V para uma lista de colunas.\n",
    "    \"\"\"\n",
    "    n_cols = len(colunas_categoricas)\n",
    "    # Inicializa a matriz com 1s na diagonal (correla√ß√£o de uma var com ela mesma)\n",
    "    matrix = np.ones((n_cols, n_cols))\n",
    "    \n",
    "    print(f\"Iniciando indexa√ß√£o de {n_cols} colunas...\")\n",
    "    \n",
    "    # 1. Indexar todas as colunas de uma vez para ganhar performance\n",
    "    indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"skip\") for c in colunas_categoricas]\n",
    "    \n",
    "    # Aplicamos a indexa√ß√£o em cadeia\n",
    "    df_indexed = df\n",
    "    models = []\n",
    "    for indexer in indexers:\n",
    "        model = indexer.fit(df_indexed)\n",
    "        df_indexed = model.transform(df_indexed)\n",
    "        models.append(model)\n",
    "    \n",
    "    # 2. Iterar sobre os pares (apenas o tri√¢ngulo superior para economizar processamento)\n",
    "    for i in range(n_cols):\n",
    "        for j in range(i + 1, n_cols):\n",
    "            col1 = colunas_categoricas[i]\n",
    "            col2 = colunas_categoricas[j]\n",
    "            col1_idx = col1 + \"_idx\"\n",
    "            col2_idx = col2 + \"_idx\"\n",
    "            \n",
    "            print(f\"Calculando Cramer's V: {col1} vs {col2}...\")\n",
    "            \n",
    "            # Preparar dados para o ChiSquareTest\n",
    "            assembler = VectorAssembler(inputCols=[col1_idx], outputCol=\"features\")\n",
    "            test_data = assembler.transform(df_indexed).select(\"features\", col2_idx)\n",
    "            \n",
    "            # Executar teste\n",
    "            res = ChiSquareTest.test(test_data, \"features\", col2_idx).head()\n",
    "            chi2 = float(res.statistics[0])\n",
    "            n = test_data.count()\n",
    "            \n",
    "            # N√∫mero de categorias de cada coluna (r e k) extra√≠dos dos modelos indexadores\n",
    "            r = len(models[i].labels)\n",
    "            k = len(models[j].labels)\n",
    "            \n",
    "            # C√°lculo do V de Cramer\n",
    "            if n > 0 and min(r-1, k-1) > 0:\n",
    "                v = np.sqrt(chi2 / (n * min(r-1, k-1)))\n",
    "            else:\n",
    "                v = 0.0\n",
    "                \n",
    "            # Preencher a matriz de forma sim√©trica\n",
    "            matrix[i, j] = v\n",
    "            matrix[j, i] = v\n",
    "            \n",
    "    # Criar DataFrame Pandas para facilitar a visualiza√ß√£o\n",
    "    cramer_df = pd.DataFrame(matrix, index=colunas_categoricas, columns=colunas_categoricas)\n",
    "    \n",
    "    # Plotar o Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cramer_df, annot=True, cmap='YlGnBu', fmt='.2f', vmin=0, vmax=1)\n",
    "    plt.title(\"Matriz de Associa√ß√£o: Cramer's V (Vari√°veis Categ√≥ricas)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return cramer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_outliers(df, coluna, negativo=True, minimo_definido=0):\n",
    "    # Calcula os quantis de forma aproximada para performance\n",
    "    quantis = df.approxQuantile(coluna, [0.25, 0.75], 0.05)\n",
    "    q1, q3 = quantis[0], quantis[1]\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    limite_superior = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Se negativo for True, permite valores negativos no limite inferior --> dependendo da vari√°vel\n",
    "    if negativo:\n",
    "        limite_inferior = q1 - 1.5 * iqr\n",
    "    else:\n",
    "        limite_inferior = max(0, (q1 - 1.5 * iqr))\n",
    "\n",
    "    if minimo_definido != 0:\n",
    "        limite_inferior = minimo_definido\n",
    "    \n",
    "    outliers = df.filter((F.col(coluna) < limite_inferior) | (F.col(coluna) > limite_superior))\n",
    "    print(f\"Vari√°vel {coluna}:\")\n",
    "    print(f\"Limites: [{limite_inferior}, {limite_superior}]\")\n",
    "    print(f\"Total de outliers (#): {outliers.count()}\")\n",
    "    print(f\"Total de outliers (%): {(outliers.count() / df.count()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500626da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tendencia_temporal(df, col_valor, aggregation=\"mean\", col_safra=\"safra\", categories=None):\n",
    "    \"\"\"\n",
    "    df: Spark DataFrame\n",
    "    col_valor: Vari√°vel num√©rica para calcular a m√©dia\n",
    "    col_safra: Coluna de tempo (ex: 'safra')\n",
    "    categories: Nome da coluna categ√≥rica para quebrar as linhas (ex: 'gender') ou None\n",
    "    \"\"\"\n",
    "    # Definir a lista de colunas para o agrupamento\n",
    "    group_cols = [col_safra]\n",
    "    if categories:\n",
    "        group_cols.append(categories)\n",
    "    \n",
    "    # Agrega√ß√£o no Spark por safra (e categoria, se houver) e calculamos a m√©dia. Transformar em Pandas para plotar\n",
    "    if aggregation == \"mean\":\n",
    "        stats_safra = (df.groupBy(group_cols)\n",
    "                        .agg(F.mean(col_valor).alias(f\"{aggregation}\"))\n",
    "                        .orderBy(col_safra)\n",
    "                        .toPandas())\n",
    "    elif aggregation == \"sum\":\n",
    "        stats_safra = (df.groupBy(group_cols)\n",
    "                        .agg(F.sum(col_valor).alias(f\"{aggregation}\"))\n",
    "                        .orderBy(col_safra)\n",
    "                        .toPandas())\n",
    "    elif aggregation == \"max\":\n",
    "        stats_safra = (df.groupBy(group_cols)\n",
    "                        .agg(F.max(col_valor).alias(f\"{aggregation}\"))\n",
    "                        .orderBy(col_safra)\n",
    "                        .toPandas())\n",
    "    elif aggregation == \"min\":\n",
    "        stats_safra = (df.groupBy(group_cols)\n",
    "                        .agg(F.min(col_valor).alias(f\"{aggregation}\"))\n",
    "                        .orderBy(col_safra)\n",
    "                        .toPandas())\n",
    "    elif aggregation == \"med\":\n",
    "        stats_safra = (df.groupBy(group_cols)\n",
    "                        .agg(F.expr(\"percentile_approx({}, 0.5)\".format(col_valor)).alias(f\"{aggregation}\"))\n",
    "                        .orderBy(col_safra)\n",
    "                        .toPandas())\n",
    "    \n",
    "    # Tratar a safra como categoria (texto) \n",
    "    stats_safra[col_safra] = stats_safra[col_safra].astype(str)\n",
    "    \n",
    "    # 4. Configura√ß√£o do Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Se 'categories' for informada, o Seaborn cria uma linha para cada categoria usando 'hue'\n",
    "    sns.lineplot(\n",
    "        data=stats_safra, \n",
    "        x=col_safra, \n",
    "        y=f\"{aggregation}\", \n",
    "        hue=categories if categories else None, \n",
    "        marker='o', \n",
    "        sort=False # Garante que a ordem das safras do Spark seja mantida\n",
    "    )\n",
    "    \n",
    "    # Ajustes finos de visualiza√ß√£o\n",
    "    titulo = f\"{aggregation} de {col_valor} por {col_safra}\"\n",
    "    if categories:\n",
    "        titulo += f\" (Quebrado por {categories})\"\n",
    "        plt.legend(title=categories, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "        \n",
    "    plt.title(titulo)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e27c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integridade_entre_bases(df_base, df_comparacao, nome_relacao, chave=[\"msno\", \"safra\"]):\n",
    "    \"\"\"\n",
    "    Rastreia a perda de dados entre tabelas (ex: membros vs logs).\n",
    "    Essencial para justificar a volumetria final do modelo.\n",
    "    \"\"\"\n",
    "    total_base = df_base.select(chave).distinct().count()\n",
    "    com_match = df_base.join(df_comparacao, chave, \"inner\").select(chave).distinct().count()\n",
    "    perda = ((total_base - com_match) / total_base) * 100\n",
    "    \n",
    "    print(f\"--- Integridade: {nome_relacao} ---\")\n",
    "    print(f\"Registros na base: {total_base} | Registros correspondentes: {com_match}\")\n",
    "    print(f\"Taxa de Perda: {perda:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(df, coluna_categorica, coluna_alvo, agrupar_por_safra=False, table=False):\n",
    "    \"\"\"\n",
    "    Plota boxplots utilizando estat√≠sticas calculadas no Spark, incluindo a m√©dia.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Definimos as colunas de agrupamento\n",
    "    cols_base = ['safra'] if agrupar_por_safra else []\n",
    "\n",
    "    for col_cat in coluna_categorica:\n",
    "        print(f\"Processando estat√≠sticas para: {col_cat}...\")\n",
    "        \n",
    "        # 2. Calculamos os quartis E a m√©dia no Spark\n",
    "        df_stats = (df\n",
    "                    .groupBy(cols_base + [col_cat])\n",
    "                    .agg(\n",
    "                        F.percentile_approx(coluna_alvo, [0.0, 0.25, 0.5, 0.75, 1.0], 10000).alias(\"stats\"),\n",
    "                        F.mean(coluna_alvo).alias(\"avg\") # <-- Adicionado m√©dia\n",
    "                    )\n",
    "                    .select(*cols_base, col_cat,\n",
    "                            F.col(\"stats\")[0].alias(\"min\"),\n",
    "                            F.col(\"stats\")[1].alias(\"q1\"),\n",
    "                            F.col(\"stats\")[2].alias(\"med\"),\n",
    "                            F.col(\"stats\")[3].alias(\"q3\"),\n",
    "                            F.col(\"stats\")[4].alias(\"max\"),\n",
    "                            F.col(\"avg\").alias(\"mean\")) # <-- Selecionando a m√©dia\n",
    "                    .toPandas())\n",
    "\n",
    "        # 3. L√≥gica de Plotagem\n",
    "        if agrupar_por_safra:\n",
    "            safras = sorted(df_stats['safra'].unique())\n",
    "            for s in safras:\n",
    "                df_safra = df_stats[df_stats['safra'] == s]\n",
    "                _desenhar_boxplot_estatistico(df_safra, col_cat, coluna_alvo, f\"Safra {s}\", show_table=table)\n",
    "        else:\n",
    "            _desenhar_boxplot_estatistico(df_stats, col_cat, coluna_alvo, \"Vis√£o Consolidada\", show_table=table)\n",
    "\n",
    "def _desenhar_boxplot_estatistico(df_plot, col_cat, col_alvo, subtitulo, show_table=False):\n",
    "    stats_list = []\n",
    "    df_plot = df_plot.sort_values(by=col_cat)\n",
    "\n",
    "    for _, row in df_plot.iterrows():\n",
    "        stats_list.append({\n",
    "            \"label\": str(row[col_cat]),\n",
    "            \"whislo\": row[\"min\"], \n",
    "            \"q1\": row[\"q1\"],\n",
    "            \"med\": row[\"med\"],\n",
    "            \"q3\": row[\"q3\"],\n",
    "            \"whishi\": row[\"max\"],\n",
    "            \"mean\": row[\"mean\"] # <-- Matplotlib usa essa chave para desenhar a m√©dia\n",
    "        })\n",
    "\n",
    "    if not stats_list:\n",
    "        return\n",
    "\n",
    "    # Renderiza√ß√£o do Gr√°fico\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = plt.gca()\n",
    "    # showmeans=True ativa a exibi√ß√£o do marcador da m√©dia\n",
    "    ax.bxp(stats_list, showfliers=False, showmeans=True, \n",
    "           meanprops={\"marker\":\"o\", \"markerfacecolor\":\"white\", \"markeredgecolor\":\"black\", \"markersize\":\"6\"}) \n",
    "    \n",
    "    plt.title(f\"Boxplot: {col_alvo} por {col_cat} ({subtitulo})\")\n",
    "    plt.ylabel(col_alvo)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Renderiza√ß√£o da Tabela\n",
    "    if show_table:\n",
    "        print(f\"\\n--- Estat√≠sticas: {col_cat} ({subtitulo}) ---\")\n",
    "        # Adicionado \"mean\" na visualiza√ß√£o da tabela\n",
    "        cols_tabela = [col_cat, \"min\", \"q1\", \"med\", \"mean\", \"q3\", \"max\"]\n",
    "        df_resumo = df_plot[cols_tabela].copy()\n",
    "        \n",
    "        try:\n",
    "            from IPython.display import display\n",
    "            display(df_resumo)\n",
    "        except ImportError:\n",
    "            print(df_resumo.to_string(index=False))\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd1fa6",
   "metadata": {},
   "source": [
    "## 3.2. Tratamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45462da",
   "metadata": {},
   "source": [
    "### 3.2.1. Agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d61e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_logs(df_logs):\n",
    "    \"\"\"\n",
    "    Agrega a tabela de logs no n√≠vel (msno, safra).\n",
    "    \n",
    "    Premissa: A base de logs j√° vem agregada por m√™s, mas esta fun√ß√£o\n",
    "    garante robustez caso surjam duplicatas em produ√ß√£o.\n",
    "    \n",
    "    Regra de agrega√ß√£o:\n",
    "    - Vari√°veis num√©ricas de uso: SUM (acumula√ß√£o de comportamento)\n",
    "    \n",
    "    Args:\n",
    "        df_logs: DataFrame Spark com logs brutos\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame agregado no n√≠vel (msno, safra)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Colunas esperadas (contrato de dados)\n",
    "    expected_cols = [\"msno\", \"safra\", \"num_25\", \"num_50\", \"num_75\", \"num_985\", \"num_100\", \"num_unq\", \"total_secs\"]\n",
    "    \n",
    "    # Selecionar apenas colunas esperadas (prote√ß√£o contra novas features)\n",
    "    df_logs = df_logs.select(*expected_cols)\n",
    "    \n",
    "    # Verificar duplicatas\n",
    "    duplicates = df_logs.groupBy(\"msno\", \"safra\").count().filter(\"count > 1\")\n",
    "    \n",
    "    if duplicates.count() > 0:\n",
    "        print(f\"‚ö†Ô∏è LOGS: {duplicates.count()} duplicatas detectadas. Aplicando agrega√ß√£o...\")\n",
    "        \n",
    "        df_logs_agg = (\n",
    "            df_logs\n",
    "            .groupBy(\"msno\", \"safra\")\n",
    "            .agg(\n",
    "                # Vari√°veis de uso: acumula√ß√£o (SUM)\n",
    "                F.sum(\"num_25\").alias(\"num_25\"),\n",
    "                F.sum(\"num_50\").alias(\"num_50\"),\n",
    "                F.sum(\"num_75\").alias(\"num_75\"),\n",
    "                F.sum(\"num_985\").alias(\"num_985\"),\n",
    "                F.sum(\"num_100\").alias(\"num_100\"),\n",
    "                F.sum(\"num_unq\").alias(\"num_unq\"),\n",
    "                F.sum(\"total_secs\").alias(\"total_secs\")))\n",
    "        \n",
    "        print(\"‚úÖ LOGS: Agrega√ß√£o conclu√≠da.\")\n",
    "        return df_logs_agg\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ LOGS: Base j√° est√° no n√≠vel (msno, safra). Nenhuma agrega√ß√£o necess√°ria.\")\n",
    "        return df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc86337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_transactions(df_transactions):\n",
    "    \"\"\"\n",
    "    Agrega a tabela de transa√ß√µes no n√≠vel (msno, safra).\n",
    "    \n",
    "    Premissa: Podem existir m√∫ltiplas transa√ß√µes por cliente no mesmo m√™s\n",
    "    (upgrades, downgrades, reprocessamentos).\n",
    "    \n",
    "    Regras de agrega√ß√£o:\n",
    "    - Flags: MAX (ocorreu ao menos uma vez?)\n",
    "    - Valores monet√°rios: SUM (acumula√ß√£o financeira)\n",
    "    - Atributos de plano e categ√≥ricas: LAST (√∫ltimo estado observado)\n",
    "    - Datas: MAX para transaction_date, LAST para membership_expire_date\n",
    "    \n",
    "    Args:\n",
    "        df_transactions: DataFrame Spark com transa√ß√µes brutas\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame agregado no n√≠vel (msno, safra)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Colunas esperadas (contrato de dados)\n",
    "    expected_cols = [\"msno\", \"safra\", \"payment_method_id\", \"payment_plan_days\", \"plan_list_price\",\n",
    "        \"actual_amount_paid\", \"is_auto_renew\", \"transaction_date\", \"membership_expire_date\", \"is_cancel\"]\n",
    "    \n",
    "    # Selecionar apenas colunas esperadas (prote√ß√£o contra novas features)\n",
    "    df_transactions = df_transactions.select(*expected_cols)\n",
    "    \n",
    "    # Verificar duplicatas\n",
    "    duplicates = df_transactions.groupBy(\"msno\", \"safra\").count().filter(\"count > 1\")\n",
    "    \n",
    "    if duplicates.count() > 0:\n",
    "        print(f\"‚ö†Ô∏è TRANSACTIONS: {duplicates.count()} duplicatas detectadas. Aplicando agrega√ß√£o...\")\n",
    "        \n",
    "        # Criar ranking por data (√∫ltima transa√ß√£o = 1)\n",
    "        w_order = Window.partitionBy(\"msno\", \"safra\").orderBy(F.col(\"transaction_date\").desc())\n",
    "        df_transactions = df_transactions.withColumn(\"_rank\", F.row_number().over(w_order))\n",
    "        \n",
    "        df_tx_agg = (\n",
    "            df_transactions\n",
    "            .groupBy(\"msno\", \"safra\")\n",
    "            .agg(\n",
    "                # Flags: MAX (ocorr√™ncia)\n",
    "                F.max(\"is_auto_renew\").alias(\"is_auto_renew\"),\n",
    "                F.max(\"is_cancel\").alias(\"is_cancel\"),\n",
    "                \n",
    "                # Valores monet√°rios: SUM (acumula√ß√£o)\n",
    "                F.sum(\"actual_amount_paid\").alias(\"actual_amount_paid\"),\n",
    "                \n",
    "                # Atributos de plano: LAST (√∫ltimo estado)\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"payment_method_id\"))).alias(\"payment_method_id\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"payment_plan_days\"))).alias(\"payment_plan_days\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"plan_list_price\"))).alias(\"plan_list_price\"),\n",
    "                \n",
    "                # Datas\n",
    "                F.max(\"transaction_date\").alias(\"transaction_date\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"membership_expire_date\"))).alias(\"membership_expire_date\")\n",
    "            ))\n",
    "        \n",
    "        print(\"‚úÖ TRANSACTIONS: Agrega√ß√£o conclu√≠da.\")\n",
    "        return df_tx_agg\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ TRANSACTIONS: Base j√° est√° no n√≠vel (msno, safra). Nenhuma agrega√ß√£o necess√°ria.\")\n",
    "        return df_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d3151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agregar_members(df_members):\n",
    "    \"\"\"\n",
    "    Agrega a tabela de membros no n√≠vel (msno, safra).\n",
    "    \n",
    "    Premissa: Atributos de membros s√£o majoritariamente est√°ticos, mas podem\n",
    "    existir m√∫ltiplas linhas por cliente em casos de mudan√ßa de cidade, \n",
    "    reativa√ß√£o, ou inconsist√™ncias de dados.\n",
    "    \n",
    "    Regras de agrega√ß√£o:\n",
    "    - Atributos demogr√°ficos: LAST (√∫ltimo estado observado)\n",
    "    - Flag de status: MAX (esteve ativo ao menos uma vez?)\n",
    "    - Data de registro: MIN (primeira ocorr√™ncia)\n",
    "    \n",
    "    Args:\n",
    "        df_members: DataFrame Spark com dados de membros\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame agregado no n√≠vel (msno, safra)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Colunas esperadas (contrato de dados)\n",
    "    expected_cols = [\"msno\", \"safra\",\"registration_init_time\", \"city\", \"bd\", \"gender\", \"registered_via\", \"is_ativo\"]\n",
    "    \n",
    "    # Selecionar apenas colunas esperadas (prote√ß√£o contra novas features)\n",
    "    df_members = df_members.select(*expected_cols)\n",
    "    \n",
    "    # Converter safra para integer (est√° como string no schema)\n",
    "    df_members = df_members.withColumn(\"safra\", F.col(\"safra\").cast(\"integer\"))\n",
    "    \n",
    "    # Verificar duplicatas\n",
    "    duplicates = df_members.groupBy(\"msno\", \"safra\").count().filter(\"count > 1\")\n",
    "    \n",
    "    if duplicates.count() > 0:\n",
    "        print(f\"‚ö†Ô∏è MEMBERS: {duplicates.count()} duplicatas detectadas. Aplicando agrega√ß√£o...\")\n",
    "        \n",
    "        # Criar ranking por data de registro (mais recente = 1)\n",
    "        # Se n√£o houver coluna de atualiza√ß√£o, usamos registration_init_time como proxy\n",
    "        w_order = Window.partitionBy(\"msno\", \"safra\").orderBy(F.col(\"registration_init_time\").desc())\n",
    "        df_members = df_members.withColumn(\"_rank\", F.row_number().over(w_order))\n",
    "        \n",
    "        df_members_agg = (\n",
    "            df_members\n",
    "            .groupBy(\"msno\", \"safra\")\n",
    "            .agg(\n",
    "                # Data de registro: MIN (primeira ocorr√™ncia)\n",
    "                F.min(\"registration_init_time\").alias(\"registration_init_time\"),\n",
    "                \n",
    "                # Atributos demogr√°ficos: LAST (√∫ltimo estado)\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"city\"))).alias(\"city\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"bd\"))).alias(\"bd\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"gender\"))).alias(\"gender\"),\n",
    "                F.max(F.when(F.col(\"_rank\") == 1, F.col(\"registered_via\"))).alias(\"registered_via\"),\n",
    "                \n",
    "                # Flag de status: MAX (esteve ativo?)\n",
    "                F.max(\"is_ativo\").alias(\"is_ativo\")))\n",
    "        \n",
    "        print(\"‚úÖ MEMBERS: Agrega√ß√£o conclu√≠da.\")\n",
    "        return df_members_agg\n",
    "    \n",
    "    else:\n",
    "        print(\"‚úÖ MEMBERS: Base j√° est√° no n√≠vel (msno, safra). Nenhuma agrega√ß√£o necess√°ria.\")\n",
    "        return df_members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf57699",
   "metadata": {},
   "source": [
    "### 3.2.2. Outliers e Categoriza√ß√£o Estat√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_winsorizacao(df, colunas, p_inf=0.01, p_sup=0.99):\n",
    "    for col in colunas:\n",
    "        # Calcula os limites baseados em percentis\n",
    "        limites = df.approxQuantile(col, [p_inf, p_sup], 0.001) # 0.001 √© a precis√£o/erro aceit√°vel\n",
    "        inf, sup = limites[0], limites[1]\n",
    "        \n",
    "        print(f\"Coluna {col}: Limite Inferior={inf}, Limite Superior={sup}\")\n",
    "        \n",
    "        # Aplica o Capping\n",
    "        df = df.withColumn(f\"{col}_win\", \n",
    "            F.when(F.col(col) < inf, inf)\n",
    "             .when(F.col(col) > sup, sup)\n",
    "             .otherwise(F.col(col))\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_by_percentile(df, col_to_group, table_name, num_buckets=4):\n",
    "    \"\"\"\n",
    "    Categoriza uma vari√°vel dinamicamente com base em percentis calculados no Spark.\n",
    "    \n",
    "    Par√¢metros:\n",
    "    - df: DataFrame do Spark.\n",
    "    - col_to_group: Nome da coluna num√©rica a ser segmentada (ex: 'total_plays').\n",
    "    - table_name: Nome da tabela de origem para compor a flag (ex: 'logs').\n",
    "    - num_buckets: N√∫mero de divis√µes desejadas. \n",
    "                   4 para Quartis (25%, 50%, 75%, 100%)\n",
    "                   10 para Decis√µes (10%, 20%, ..., 100%)\n",
    "    \n",
    "    Vari√°veis esperadas no DataFrame:\n",
    "    - f\"flag_has_{table_name}\": Coluna bin√°ria (0/1) que indica presen√ßa de dados.\n",
    "    - {col_to_group}: A vari√°vel num√©rica que ser√° processada.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Definimos a coluna de flag\n",
    "    flag_col = f\"flag_has_{table_name}\"\n",
    "    \n",
    "    # 2. Calculo dinamico dos percentis\n",
    "    # Criamos uma lista de probabilidades: ex para 4 buckets -> [0.25, 0.50, 0.75, 1.0]\n",
    "    probabilities = [i / num_buckets for i in range(1, num_buckets + 1)]\n",
    "    \n",
    "    # Coletamos os valores de corte (stats)\n",
    "    # Nota: Filtramos apenas onde a flag √© 1 para n√£o enviesar os percentis com zeros/nulos\n",
    "    cut_off_points = (df\n",
    "                      .filter(F.col(flag_col).isin(1))\n",
    "                      .agg(F.percentile_approx(col_to_group, probabilities).alias(\"pts\"))\n",
    "                      .collect()[0][\"pts\"])\n",
    "    \n",
    "    # 3. A l√≥gica do .when dinamico, comecando com a condicao base para desconhecidos/inativos\n",
    "    case_expression = F.when(F.col(flag_col).isin(0), \"unknown\")\n",
    "    \n",
    "    # Iteramos sobre os pontos de corte para criar as faixas\n",
    "    # Ex: para Quartis, teremos Tier 1 (at√© 25%), Tier 2 (at√© 50%), etc.\n",
    "    for i, point in enumerate(cut_off_points):\n",
    "        tier_label = f\"tier_{i+1}\"\n",
    "        # A primeira faixa pega do menor valor at√© o primeiro ponto\n",
    "        if i == 0:\n",
    "            case_expression = case_expression.when(F.col(col_to_group) <= point, tier_label)\n",
    "        else:\n",
    "            # As demais pegam entre o ponto anterior e o atual\n",
    "            case_expression = case_expression.when(\n",
    "                (F.col(col_to_group) > cut_off_points[i-1]) & (F.col(col_to_group) <= point), \n",
    "                tier_label)\n",
    "            \n",
    "    # Caso escape de alguma l√≥gica (fallback)\n",
    "    case_expression = case_expression.otherwise(f\"tier_{num_buckets}\")\n",
    "\n",
    "    # Retornamos o dataframe com a nova coluna\n",
    "    return df.withColumn(f\"{col_to_group}_group\", case_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fae186",
   "metadata": {},
   "source": [
    "### 3.2.3. Tend√™ncia - Janela temporal (3 e 6 meses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_de_tendencia_num(df, variables, id_col=\"msno\", time_col=\"safra\", windows=[3, 6], keep_intermediate=False):\n",
    "    original_cols = df.columns\n",
    "    generated_cols = []\n",
    "\n",
    "    for var in variables:\n",
    "        # manter raw\n",
    "        raw_col = f\"{var}_raw\"\n",
    "        df = df.withColumn(raw_col, F.col(var))\n",
    "        generated_cols.append(raw_col)\n",
    "\n",
    "        for n in windows:\n",
    "            w = Window.partitionBy(id_col).orderBy(time_col)\n",
    "\n",
    "            # lags\n",
    "            lags = []\n",
    "            for i in range(n):\n",
    "                lag_col = f\"{var}_lag_{i}\"\n",
    "                df = df.withColumn(lag_col, F.lag(var, i).over(w))\n",
    "                lags.append(lag_col)\n",
    "\n",
    "            # contadores\n",
    "            cnt_null = f\"{var}_cnt_null_{n}\"\n",
    "            cnt_zero = f\"{var}_cnt_zero_{n}\"\n",
    "\n",
    "            df = (df\n",
    "                .withColumn(cnt_null, sum(F.col(c).isNull().cast(\"int\") for c in lags))\n",
    "                .withColumn(cnt_zero, sum((F.col(c) == 0).cast(\"int\") for c in lags)))\n",
    "\n",
    "            ref = f\"{var}_lag_0\"\n",
    "            arr = f\"array({','.join(lags)})\"\n",
    "\n",
    "            # m√©tricas raw\n",
    "            mean_raw = f\"{var}_mean_{n}_raw\"\n",
    "            min_raw = f\"{var}_min_{n}_raw\"\n",
    "            max_raw = f\"{var}_max_{n}_raw\"\n",
    "\n",
    "            df = (df\n",
    "                .withColumn(\n",
    "                    mean_raw,\n",
    "                    F.expr(f\"\"\"\n",
    "                        aggregate(filter({arr}, x -> x is not null), 0D, (acc, x) -> acc + x) / size(filter({arr}, x -> x is not null))\n",
    "                        \"\"\"))\n",
    "                .withColumn(min_raw, F.expr(f\"array_min(filter({arr}, x -> x is not null))\"))\n",
    "                .withColumn(max_raw, F.expr(f\"array_max(filter({arr}, x -> x is not null))\")))\n",
    "\n",
    "            # m√©tricas finais (com sentinela)\n",
    "            for metric, raw_metric in zip(\n",
    "                [\"mean\", \"min\", \"max\"],\n",
    "                [mean_raw, min_raw, max_raw]\n",
    "            ):\n",
    "                final_col = f\"{var}_{metric}_{n}\"\n",
    "                generated_cols.append(final_col)\n",
    "\n",
    "                df = df.withColumn(final_col,\n",
    "                    F.when(F.col(cnt_null) == n, F.lit(-99998))\n",
    "                     .when(F.col(cnt_zero) == n, F.lit(-99999))\n",
    "                     .when((F.col(ref) == 0) & (F.col(cnt_null) < n), F.lit(-99995))\n",
    "                     .when((F.col(ref).isNotNull()) & (F.col(cnt_null) == n - 1), F.lit(-99996))\n",
    "                     .when((F.col(ref).isNotNull()) & (F.col(cnt_zero) == n - 1), F.lit(-99997))\n",
    "                     .otherwise(F.col(raw_metric)))\n",
    "\n",
    "            # raz√µes\n",
    "            for metric in [\"mean\", \"min\", \"max\"]:\n",
    "                denom = f\"{var}_{metric}_{n}\"\n",
    "                ratio = f\"{var}_ratio_ref_{metric}_{n}\"\n",
    "                generated_cols.append(ratio)\n",
    "\n",
    "                df = df.withColumn(ratio,\n",
    "                    F.when(F.col(denom) <= 0, F.col(denom))\n",
    "                    .otherwise(F.col(ref) / F.col(denom)))\n",
    "\n",
    "    # =========================\n",
    "    # CONTROLE DE SA√çDA\n",
    "    # =========================\n",
    "    if not keep_intermediate:\n",
    "        final_cols = list(dict.fromkeys(\n",
    "            original_cols + generated_cols))\n",
    "        df = df.select(final_cols)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c888be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_de_tendencia_cat(df, flags, id_col=\"msno\", time_col=\"safra\", windows=[3, 6]):\n",
    "    for flag in flags:\n",
    "        df = df.withColumn(f\"{flag}_raw\", F.col(flag))\n",
    "\n",
    "        for n in windows:\n",
    "            w = Window.partitionBy(id_col).orderBy(time_col)\n",
    "\n",
    "            lags = []\n",
    "            for i in range(n):\n",
    "                lag_col = f\"{flag}_lag_{i}\"\n",
    "                df = df.withColumn(lag_col, F.lag(flag, i).over(w))\n",
    "                lags.append(lag_col)\n",
    "\n",
    "            arr = f\"array({','.join(lags)})\"\n",
    "\n",
    "            df = (df\n",
    "                .withColumn(f\"{flag}_sum_{n}\",\n",
    "                    F.expr(f\"aggregate(filter({arr}, x -> x is not null), 0, (acc, x) -> acc + x)\"))\n",
    "                .withColumn(f\"{flag}_mean_{n}\", F.col(f\"{flag}_sum_{n}\") / F.lit(n))\n",
    "                .withColumn(f\"{flag}_max_{n}\", F.expr(f\"array_max(filter({arr}, x -> x is not null))\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d321ff1b",
   "metadata": {},
   "source": [
    "#### Objetivo geral\n",
    "O objetivo desta fun√ß√£o √© capturar **comportamentos temporais dos usu√°rios**, indo al√©m do valor pontual do m√™s de refer√™ncia.  \n",
    "Para cada cliente (`msno`) e safra (`yyyyMM`), s√£o constru√≠das vari√°veis que representam:\n",
    "- o **n√≠vel atual** de uma m√©trica,\n",
    "- o **hist√≥rico recente** dessa m√©trica,\n",
    "- e a **rela√ß√£o entre o comportamento atual e o passado**.\n",
    "\n",
    "Essa abordagem permite ao modelo identificar:\n",
    "- padr√µes de crescimento ou queda,\n",
    "- usu√°rios novos vs recorrentes,\n",
    "- mudan√ßas abruptas de comportamento,\n",
    "- aus√™ncia estrutural de informa√ß√£o.\n",
    "\n",
    "---\n",
    "\n",
    "#### Estrutura geral da fun√ß√£o\n",
    "A fun√ß√£o recebe:\n",
    "- uma lista de vari√°veis cont√≠nuas (ex: `num_100`, `total_secs`, `avg_secs_per_play`);\n",
    "- janelas temporais fixas (`n = 3` e `n = 6` meses);\n",
    "- e gera automaticamente **todas as transforma√ß√µes** para todas as vari√°veis listadas.\n",
    "\n",
    "Para cada vari√°vel e cada janela temporal, s√£o criados:\n",
    "1. valores defasados (lags),\n",
    "2. m√©tricas estat√≠sticas hist√≥ricas,\n",
    "3. raz√µes entre o valor atual e o hist√≥rico,\n",
    "4. vers√µes com e sem c√≥digos sentinela,\n",
    "5. mantendo sempre o valor original (raw).\n",
    "\n",
    "---\n",
    "\n",
    "#### Lags temporais\n",
    "Para cada vari√°vel `X`, s√£o criados:\n",
    "- `X_lag_0`: valor no m√™s de refer√™ncia\n",
    "- `X_lag_1`: valor no m√™s anterior\n",
    "- ...\n",
    "- `X_lag_(n-1)`: valor n‚àí1 meses atr√°s\n",
    "\n",
    "Esses lags formam a base para todas as transforma√ß√µes seguintes.\n",
    "\n",
    "---\n",
    "\n",
    "#### Vari√°veis `*_raw`\n",
    "As vari√°veis com sufixo `_raw` representam o **valor matem√°tico puro**, sem qualquer regra sem√¢ntica adicional.\n",
    "\n",
    "Exemplos:\n",
    "- `num_100_raw`\n",
    "- `num_100_mean_6_raw`\n",
    "- `total_secs_max_3_raw`\n",
    "\n",
    "Caracter√≠sticas:\n",
    "- ignoram valores nulos quando poss√≠vel;\n",
    "- n√£o utilizam c√≥digos sentinela;\n",
    "- podem assumir valores nulos ou zero naturalmente.\n",
    "\n",
    "Uso principal:\n",
    "- regress√£o linear,\n",
    "- an√°lises estat√≠sticas,\n",
    "- inspe√ß√£o e debug.\n",
    "\n",
    "---\n",
    "\n",
    "#### M√©tricas hist√≥ricas criadas\n",
    "Para cada vari√°vel `X` e janela `n`, s√£o criadas:\n",
    "\n",
    "- `X_mean_n_raw`: m√©dia dos √∫ltimos `n` meses\n",
    "- `X_min_n_raw`: m√≠nimo dos √∫ltimos `n` meses\n",
    "- `X_max_n_raw`: m√°ximo dos √∫ltimos `n` meses\n",
    "\n",
    "Essas m√©tricas descrevem o **n√≠vel t√≠pico**, o **pior cen√°rio** e o **melhor cen√°rio recente** do usu√°rio.\n",
    "\n",
    "---\n",
    "\n",
    "#### Vari√°veis sem `_raw` (com sem√¢ntica)\n",
    "As vari√°veis sem o sufixo `_raw` aplicam **regras sem√¢nticas** para diferenciar situa√ß√µes estruturalmente distintas, que numericamente poderiam parecer iguais.\n",
    "\n",
    "Essas vari√°veis utilizam **c√≥digos sentinela negativos**, que carregam significado comportamental:\n",
    "\n",
    "| C√≥digo | Significado |\n",
    "|------|-------------|\n",
    "| `-99998` | N√£o h√° registros em nenhum dos √∫ltimos `n` meses |\n",
    "| `-99999` | Todos os valores nos √∫ltimos `n` meses s√£o zero |\n",
    "| `-99995` | Valor atual √© zero, mas havia hist√≥rico v√°lido |\n",
    "| `-99996` | Valor atual existe, mas hist√≥rico √© todo nulo |\n",
    "| `-99997` | Valor atual existe, mas hist√≥rico √© todo zero |\n",
    "\n",
    "Esses c√≥digos permitem ao modelo (especialmente √°rvores):\n",
    "- distinguir aus√™ncia de uso vs aus√™ncia de dados,\n",
    "- identificar usu√°rios novos,\n",
    "- detectar interrup√ß√µes de comportamento.\n",
    "\n",
    "---\n",
    "\n",
    "#### Raz√µes (vari√°veis de mudan√ßa de comportamento)\n",
    "Para cada m√©trica hist√≥rica, s√£o criadas raz√µes do tipo:\n",
    "\n",
    "- `X_ratio_ref_mean_n`\n",
    "- `X_ratio_ref_min_n`\n",
    "- `X_ratio_ref_max_n`\n",
    "\n",
    "Essas vari√°veis comparam o valor atual (`lag_0`) com o hist√≥rico recente.\n",
    "\n",
    "Interpreta√ß√£o:\n",
    "- valor ‚âà 1 ‚Üí comportamento est√°vel\n",
    "- valor > 1 ‚Üí aumento recente\n",
    "- valor < 1 ‚Üí queda recente\n",
    "\n",
    "Caso o denominador seja inv√°lido (zero, nulo ou sentinela), a raz√£o herda o c√≥digo sentinela, evitando divis√µes inv√°lidas.\n",
    "\n",
    "---\n",
    "\n",
    "#### Separa√ß√£o entre estat√≠stica e sem√¢ntica\n",
    "Um ponto central do design √© **n√£o misturar estat√≠stica com sem√¢ntica**.\n",
    "\n",
    "Por isso:\n",
    "- vari√°veis `_raw` s√£o mantidas limpas e cont√≠nuas;\n",
    "- vari√°veis sem `_raw` carregam contexto comportamental.\n",
    "\n",
    "Essa separa√ß√£o permite:\n",
    "- uso seguro em regress√£o linear (via `_raw`);\n",
    "- explora√ß√£o rica de padr√µes em modelos baseados em √°rvore (via sentinelas).\n",
    "\n",
    "---\n",
    "\n",
    "#### Vari√°veis de flags (bin√°rias)\n",
    "Para vari√°veis categ√≥ricas/bin√°rias (ex: `is_cancel`, `is_auto_renew`), s√£o criadas transforma√ß√µes espec√≠ficas:\n",
    "\n",
    "- `flag_sum_n`: n√∫mero de ocorr√™ncias nos √∫ltimos `n` meses\n",
    "- `flag_mean_n`: frequ√™ncia relativa\n",
    "- `flag_max_n`: ocorreu pelo menos uma vez\n",
    "\n",
    "Essas vari√°veis capturam **persist√™ncia, recorr√™ncia e reincid√™ncia** de eventos.\n",
    "\n",
    "---\n",
    "\n",
    "#### Benef√≠cios da abordagem\n",
    "Essa arquitetura de features:\n",
    "- reduz depend√™ncia de um √∫nico m√™s,\n",
    "- melhora estabilidade temporal,\n",
    "- captura mudan√ßas de comportamento,\n",
    "- √© robusta a dados faltantes,\n",
    "- funciona bem tanto para regress√£o quanto para √°rvores.\n",
    "\n",
    "Al√©m disso, permite sele√ß√£o posterior de vari√°veis sem refazer o pipeline, mantendo rastreabilidade e flexibilidade.\n",
    "\n",
    "---\n",
    "\n",
    "#### Observa√ß√£o final\n",
    "Nem todas as vari√°veis criadas necessariamente entrar√£o no modelo final.  \n",
    "A fun√ß√£o foi desenhada para **gerar um espa√ßo rico de candidatos**, permitindo que a sele√ß√£o seja feita de forma emp√≠rica, baseada em valida√ß√£o e performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
